{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"jQUtvMvYsM-T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700014839229,"user_tz":-630,"elapsed":19335,"user":{"displayName":"Sanchi","userId":"16319292524849164681"}},"outputId":"eacd3c55-a004-4ae7-ef65-57e23f8d8511"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!unzip /content/drive/MyDrive/DAN.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"onwhsRGv2suJ","executionInfo":{"status":"ok","timestamp":1700014841566,"user_tz":-630,"elapsed":2343,"user":{"displayName":"Sanchi","userId":"16319292524849164681"}},"outputId":"3fd2b278-9d70-4dd2-c3ce-5ad339ac9fe4"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  /content/drive/MyDrive/DAN.zip\n","   creating: content/DAN/\n","  inflating: content/DAN/README.md   \n","  inflating: content/DAN/LICENSE_CECILL-C.md  \n","  inflating: content/DAN/visual_slanted_lines.png  \n","   creating: content/DAN/.git/\n","   creating: content/DAN/.git/branches/\n","  inflating: content/DAN/.git/packed-refs  \n"," extracting: content/DAN/.git/HEAD   \n","   creating: content/DAN/.git/hooks/\n","  inflating: content/DAN/.git/hooks/pre-merge-commit.sample  \n","  inflating: content/DAN/.git/hooks/update.sample  \n","  inflating: content/DAN/.git/hooks/post-update.sample  \n","  inflating: content/DAN/.git/hooks/pre-push.sample  \n","  inflating: content/DAN/.git/hooks/applypatch-msg.sample  \n","  inflating: content/DAN/.git/hooks/pre-applypatch.sample  \n","  inflating: content/DAN/.git/hooks/commit-msg.sample  \n","  inflating: content/DAN/.git/hooks/prepare-commit-msg.sample  \n","  inflating: content/DAN/.git/hooks/push-to-checkout.sample  \n","  inflating: content/DAN/.git/hooks/pre-receive.sample  \n","  inflating: content/DAN/.git/hooks/pre-rebase.sample  \n","  inflating: content/DAN/.git/hooks/fsmonitor-watchman.sample  \n","  inflating: content/DAN/.git/hooks/pre-commit.sample  \n","   creating: content/DAN/.git/refs/\n","   creating: content/DAN/.git/refs/remotes/\n","   creating: content/DAN/.git/refs/remotes/origin/\n"," extracting: content/DAN/.git/refs/remotes/origin/HEAD  \n","   creating: content/DAN/.git/refs/heads/\n"," extracting: content/DAN/.git/refs/heads/main  \n","   creating: content/DAN/.git/refs/tags/\n","  inflating: content/DAN/.git/index  \n","  inflating: content/DAN/.git/config  \n","   creating: content/DAN/.git/info/\n","  inflating: content/DAN/.git/info/exclude  \n","  inflating: content/DAN/.git/description  \n","   creating: content/DAN/.git/objects/\n","   creating: content/DAN/.git/objects/pack/\n","  inflating: content/DAN/.git/objects/pack/pack-03f0acc3a5342eedb6a8f3e71e0d5aed9a302263.idx  \n","  inflating: content/DAN/.git/objects/pack/pack-03f0acc3a5342eedb6a8f3e71e0d5aed9a302263.pack  \n","   creating: content/DAN/.git/objects/info/\n","   creating: content/DAN/.git/logs/\n","  inflating: content/DAN/.git/logs/HEAD  \n","   creating: content/DAN/.git/logs/refs/\n","   creating: content/DAN/.git/logs/refs/remotes/\n","   creating: content/DAN/.git/logs/refs/remotes/origin/\n","  inflating: content/DAN/.git/logs/refs/remotes/origin/HEAD  \n","   creating: content/DAN/.git/logs/refs/heads/\n","  inflating: content/DAN/.git/logs/refs/heads/main  \n","   creating: content/DAN/Datasets/\n","   creating: content/DAN/Datasets/formatted/\n","   creating: content/DAN/Datasets/formatted/READ_2016_page_sem/\n","   creating: content/DAN/Datasets/formatted/READ_2016_page_sem/temp/\n","   creating: content/DAN/Datasets/dataset_formatters/\n","  inflating: content/DAN/Datasets/dataset_formatters/generic_dataset_formatter.py  \n","  inflating: content/DAN/Datasets/dataset_formatters/rimes_formatter.py  \n","   creating: content/DAN/Datasets/dataset_formatters/__pycache__/\n","  inflating: content/DAN/Datasets/dataset_formatters/__pycache__/generic_dataset_formatter.cpython-310.pyc  \n","  inflating: content/DAN/Datasets/dataset_formatters/utils_dataset.py  \n","  inflating: content/DAN/Datasets/dataset_formatters/read2016_formatter.py  \n","  inflating: content/DAN/requirements.txt  \n","  inflating: content/DAN/visual.png  \n","  inflating: content/DAN/.gitignore  \n","   creating: content/DAN/OCR/\n","  inflating: content/DAN/OCR/ocr_manager.py  \n","  inflating: content/DAN/OCR/ocr_dataset_manager.py  \n","   creating: content/DAN/OCR/line_OCR/\n","   creating: content/DAN/OCR/line_OCR/ctc/\n","  inflating: content/DAN/OCR/line_OCR/ctc/main_line_ctc.py  \n","  inflating: content/DAN/OCR/line_OCR/ctc/trainer_line_ctc.py  \n","  inflating: content/DAN/OCR/line_OCR/ctc/main_syn_line.py  \n","  inflating: content/DAN/OCR/line_OCR/ctc/models_line_ctc.py  \n","   creating: content/DAN/OCR/document_OCR/\n","   creating: content/DAN/OCR/document_OCR/dan/\n","  inflating: content/DAN/OCR/document_OCR/dan/trainer_dan.py  \n","  inflating: content/DAN/OCR/document_OCR/dan/predict_example.py  \n","  inflating: content/DAN/OCR/document_OCR/dan/main_dan.py  \n","  inflating: content/DAN/OCR/document_OCR/dan/models_dan.py  \n","  inflating: content/DAN/OCR/ocr_utils.py  \n","   creating: content/DAN/basic/\n","  inflating: content/DAN/basic/scheduler.py  \n","  inflating: content/DAN/basic/post_pocessing_layout.py  \n","  inflating: content/DAN/basic/generic_dataset_manager.py  \n","  inflating: content/DAN/basic/models.py  \n","  inflating: content/DAN/basic/generic_training_manager.py  \n","  inflating: content/DAN/basic/metric_manager.py  \n","  inflating: content/DAN/basic/utils.py  \n","  inflating: content/DAN/basic/transforms.py  \n","   creating: content/DAN/Fonts/\n","  inflating: content/DAN/Fonts/list_fonts_rimes.txt  \n","  inflating: content/DAN/Fonts/list_fonts_read_2016.txt  \n","   creating: content/content/DAN/\n","  inflating: content/content/DAN/README.md  \n","   creating: content/content/DAN/outputs/\n","   creating: content/content/DAN/outputs/dan_rimes_page/\n","   creating: content/content/DAN/outputs/dan_rimes_page/checkpoints/\n","   creating: content/content/DAN/outputs/dan_rimes_page/results/\n","  inflating: content/content/DAN/LICENSE_CECILL-C.md  \n","  inflating: content/content/DAN/generic_dataset_formatter.py  \n","  inflating: content/content/DAN/visual_slanted_lines.png  \n","   creating: content/content/DAN/.git/\n","   creating: content/content/DAN/.git/branches/\n","  inflating: content/content/DAN/.git/packed-refs  \n"," extracting: content/content/DAN/.git/HEAD  \n","   creating: content/content/DAN/.git/hooks/\n","  inflating: content/content/DAN/.git/hooks/pre-merge-commit.sample  \n","  inflating: content/content/DAN/.git/hooks/update.sample  \n","  inflating: content/content/DAN/.git/hooks/post-update.sample  \n","  inflating: content/content/DAN/.git/hooks/pre-push.sample  \n","  inflating: content/content/DAN/.git/hooks/applypatch-msg.sample  \n","  inflating: content/content/DAN/.git/hooks/pre-applypatch.sample  \n","  inflating: content/content/DAN/.git/hooks/commit-msg.sample  \n","  inflating: content/content/DAN/.git/hooks/prepare-commit-msg.sample  \n","  inflating: content/content/DAN/.git/hooks/push-to-checkout.sample  \n","  inflating: content/content/DAN/.git/hooks/pre-receive.sample  \n","  inflating: content/content/DAN/.git/hooks/pre-rebase.sample  \n","  inflating: content/content/DAN/.git/hooks/fsmonitor-watchman.sample  \n","  inflating: content/content/DAN/.git/hooks/pre-commit.sample  \n","   creating: content/content/DAN/.git/refs/\n","   creating: content/content/DAN/.git/refs/remotes/\n","   creating: content/content/DAN/.git/refs/remotes/origin/\n"," extracting: content/content/DAN/.git/refs/remotes/origin/HEAD  \n","   creating: content/content/DAN/.git/refs/heads/\n"," extracting: content/content/DAN/.git/refs/heads/main  \n","   creating: content/content/DAN/.git/refs/tags/\n","  inflating: content/content/DAN/.git/index  \n","  inflating: content/content/DAN/.git/config  \n","   creating: content/content/DAN/.git/info/\n","  inflating: content/content/DAN/.git/info/exclude  \n","  inflating: content/content/DAN/.git/description  \n","   creating: content/content/DAN/.git/objects/\n","   creating: content/content/DAN/.git/objects/pack/\n","  inflating: content/content/DAN/.git/objects/pack/pack-03f0acc3a5342eedb6a8f3e71e0d5aed9a302263.idx  \n","  inflating: content/content/DAN/.git/objects/pack/pack-03f0acc3a5342eedb6a8f3e71e0d5aed9a302263.pack  \n","   creating: content/content/DAN/.git/objects/info/\n","   creating: content/content/DAN/.git/logs/\n","  inflating: content/content/DAN/.git/logs/HEAD  \n","   creating: content/content/DAN/.git/logs/refs/\n","   creating: content/content/DAN/.git/logs/refs/remotes/\n","   creating: content/content/DAN/.git/logs/refs/remotes/origin/\n","  inflating: content/content/DAN/.git/logs/refs/remotes/origin/HEAD  \n","   creating: content/content/DAN/.git/logs/refs/heads/\n","  inflating: content/content/DAN/.git/logs/refs/heads/main  \n","   creating: content/content/DAN/__pycache__/\n","  inflating: content/content/DAN/__pycache__/generic_dataset_formatter.cpython-310.pyc  \n","   creating: content/content/DAN/Datasets/\n","   creating: content/content/DAN/Datasets/formatted/\n","   creating: content/content/DAN/Datasets/formatted/READ_2016_page_sem/\n","   creating: content/content/DAN/Datasets/formatted/READ_2016_page_sem/temp/\n","   creating: content/content/DAN/Datasets/dataset_formatters/\n","  inflating: content/content/DAN/Datasets/dataset_formatters/generic_dataset_formatter.py  \n","  inflating: content/content/DAN/Datasets/dataset_formatters/rimes_formatter.py  \n","   creating: content/content/DAN/Datasets/dataset_formatters/__pycache__/\n","  inflating: content/content/DAN/Datasets/dataset_formatters/__pycache__/generic_dataset_formatter.cpython-310.pyc  \n","  inflating: content/content/DAN/Datasets/dataset_formatters/__pycache__/read2016_formatter.cpython-310.pyc  \n","  inflating: content/content/DAN/Datasets/dataset_formatters/__pycache__/utils_dataset.cpython-310.pyc  \n","  inflating: content/content/DAN/Datasets/dataset_formatters/__pycache__/rimes_formatter.cpython-310.pyc  \n","  inflating: content/content/DAN/Datasets/dataset_formatters/utils_dataset.py  \n","  inflating: content/content/DAN/Datasets/dataset_formatters/read2016_formatter.py  \n","  inflating: content/content/DAN/requirements.txt  \n","  inflating: content/content/DAN/visual.png  \n","  inflating: content/content/DAN/.gitignore  \n","   creating: content/content/DAN/OCR/\n","   creating: content/content/DAN/OCR/__pycache__/\n","  inflating: content/content/DAN/OCR/__pycache__/ocr_utils.cpython-310.pyc  \n","  inflating: content/content/DAN/OCR/__pycache__/ocr_manager.cpython-310.pyc  \n","  inflating: content/content/DAN/OCR/ocr_manager.py  \n","  inflating: content/content/DAN/OCR/ocr_dataset_manager.py  \n","   creating: content/content/DAN/OCR/line_OCR/\n","   creating: content/content/DAN/OCR/line_OCR/ctc/\n","  inflating: content/content/DAN/OCR/line_OCR/ctc/main_line_ctc.py  \n","  inflating: content/content/DAN/OCR/line_OCR/ctc/trainer_line_ctc.py  \n","  inflating: content/content/DAN/OCR/line_OCR/ctc/main_syn_line.py  \n","  inflating: content/content/DAN/OCR/line_OCR/ctc/models_line_ctc.py  \n","   creating: content/content/DAN/OCR/document_OCR/\n","   creating: content/content/DAN/OCR/document_OCR/dan/\n","  inflating: content/content/DAN/OCR/document_OCR/dan/trainer_dan.py  \n","  inflating: content/content/DAN/OCR/document_OCR/dan/predict_example.py  \n","   creating: content/content/DAN/OCR/document_OCR/dan/__pycache__/\n","  inflating: content/content/DAN/OCR/document_OCR/dan/__pycache__/models_dan.cpython-310.pyc  \n","  inflating: content/content/DAN/OCR/document_OCR/dan/__pycache__/trainer_dan.cpython-310.pyc  \n","  inflating: content/content/DAN/OCR/document_OCR/dan/main_dan.py  \n","  inflating: content/content/DAN/OCR/document_OCR/dan/models_dan.py  \n","  inflating: content/content/DAN/OCR/ocr_utils.py  \n","   creating: content/content/DAN/basic/\n","  inflating: content/content/DAN/basic/scheduler.py  \n","   creating: content/content/DAN/basic/__pycache__/\n","  inflating: content/content/DAN/basic/__pycache__/scheduler.cpython-310.pyc  \n","  inflating: content/content/DAN/basic/__pycache__/utils.cpython-310.pyc  \n","  inflating: content/content/DAN/basic/__pycache__/metric_manager.cpython-310.pyc  \n","  inflating: content/content/DAN/basic/__pycache__/post_pocessing_layout.cpython-310.pyc  \n","  inflating: content/content/DAN/basic/__pycache__/generic_training_manager.cpython-310.pyc  \n","  inflating: content/content/DAN/basic/__pycache__/models.cpython-310.pyc  \n","  inflating: content/content/DAN/basic/post_pocessing_layout.py  \n","  inflating: content/content/DAN/basic/generic_dataset_manager.py  \n","  inflating: content/content/DAN/basic/models.py  \n","  inflating: content/content/DAN/basic/generic_training_manager.py  \n","  inflating: content/content/DAN/basic/metric_manager.py  \n","  inflating: content/content/DAN/basic/utils.py  \n","  inflating: content/content/DAN/basic/transforms.py  \n","   creating: content/content/DAN/Fonts/\n","  inflating: content/content/DAN/Fonts/list_fonts_rimes.txt  \n","  inflating: content/content/DAN/Fonts/list_fonts_read_2016.txt  \n"]}]},{"cell_type":"markdown","source":["# **Dependencies Installation**"],"metadata":{"id":"f7-qK0J6yUJy"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4gxz3f9ljmqP","outputId":"c4a710b2-a3cc-45cb-cbb7-34d3a28a58c6","executionInfo":{"status":"ok","timestamp":1700015057374,"user_tz":-630,"elapsed":171084,"user":{"displayName":"Sanchi","userId":"16319292524849164681"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.3)\n","Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.23.5)\n","Requirement already satisfied: skia-pathops in /usr/local/lib/python3.10/dist-packages (0.8.0.post1)\n","Collecting tiffile\n","  Using cached tiffile-2018.10.18-py2.py3-none-any.whl (2.7 kB)\n","Requirement already satisfied: tifffile in /usr/local/lib/python3.10/dist-packages (from tiffile) (2023.9.26)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tifffile->tiffile) (1.23.5)\n","Installing collected packages: tiffile\n","Successfully installed tiffile-2018.10.18\n","Collecting srt\n","  Using cached srt-3.5.3-py3-none-any.whl\n","Installing collected packages: srt\n","Successfully installed srt-3.5.3\n","Requirement already satisfied: skia-pathops in /usr/local/lib/python3.10/dist-packages (0.8.0.post1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu118)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n","Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.1.0+cu118)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (2.1.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchvision) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchvision) (1.3.0)\n","Collecting screeninfo==0.7\n","  Downloading screeninfo-0.7.tar.gz (10 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: screeninfo\n","  Building wheel for screeninfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for screeninfo: filename=screeninfo-0.7-py3-none-any.whl size=13624 sha256=0428bfdf649b382d27be2da899971dae27785b114f471cdb8edd780146199706\n","  Stored in directory: /root/.cache/pip/wheels/02/8a/a8/3bb3aa4fd31941124c5975d00412a63938fa5879c4b7b63458\n","Successfully built screeninfo\n","Installing collected packages: screeninfo\n","Successfully installed screeninfo-0.7\n","Collecting pyunpack\n","  Downloading pyunpack-0.3-py2.py3-none-any.whl (4.1 kB)\n","Collecting easyprocess (from pyunpack)\n","  Downloading EasyProcess-1.1-py3-none-any.whl (8.7 kB)\n","Collecting entrypoint2 (from pyunpack)\n","  Downloading entrypoint2-1.1-py2.py3-none-any.whl (9.9 kB)\n","Installing collected packages: entrypoint2, easyprocess, pyunpack\n","Successfully installed easyprocess-1.1 entrypoint2-1.1 pyunpack-0.3\n","Collecting absl-py==1.0.0 (from -r /content/content/DAN/requirements.txt (line 1))\n","  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.7/126.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting backports.cached-property==1.0.1 (from -r /content/content/DAN/requirements.txt (line 2))\n","  Downloading backports.cached_property-1.0.1-py3-none-any.whl (5.7 kB)\n","Collecting cachetools==5.0.0 (from -r /content/content/DAN/requirements.txt (line 3))\n","  Downloading cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n","Collecting certifi==2021.10.8 (from -r /content/content/DAN/requirements.txt (line 4))\n","  Downloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.2/149.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting charset-normalizer==2.0.12 (from -r /content/content/DAN/requirements.txt (line 5))\n","  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n","Collecting click==8.0.4 (from -r /content/content/DAN/requirements.txt (line 6))\n","  Downloading click-8.0.4-py3-none-any.whl (97 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.5/97.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting click-default-group==1.2.2 (from -r /content/content/DAN/requirements.txt (line 7))\n","  Downloading click-default-group-1.2.2.tar.gz (3.3 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting cloup==0.7.1 (from -r /content/content/DAN/requirements.txt (line 8))\n","  Downloading cloup-0.7.1-py2.py3-none-any.whl (23 kB)\n","Collecting colorama==0.4.4 (from -r /content/content/DAN/requirements.txt (line 9))\n","  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: colour==0.1.5 in /usr/local/lib/python3.10/dist-packages (from -r /content/content/DAN/requirements.txt (line 10)) (0.1.5)\n","Collecting commonmark==0.9.1 (from -r /content/content/DAN/requirements.txt (line 11))\n","  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cycler==0.11.0 (from -r /content/content/DAN/requirements.txt (line 12))\n","  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n","Collecting decorator==5.1.1 (from -r /content/content/DAN/requirements.txt (line 13))\n","  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n","Requirement already satisfied: EasyProcess==1.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/content/DAN/requirements.txt (line 14)) (1.1)\n","Collecting editdistance==0.6.0 (from -r /content/content/DAN/requirements.txt (line 15))\n","  Downloading editdistance-0.6.0.tar.gz (29 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting entrypoint2==1.0 (from -r /content/content/DAN/requirements.txt (line 16))\n","  Downloading entrypoint2-1.0-py3-none-any.whl (9.8 kB)\n","Collecting fonttools==4.29.1 (from -r /content/content/DAN/requirements.txt (line 17))\n","  Downloading fonttools-4.29.1-py3-none-any.whl (895 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m895.5/895.5 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting glcontext==2.3.4 (from -r /content/content/DAN/requirements.txt (line 18))\n","  Downloading glcontext-2.3.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_27_x86_64.whl (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting google-auth==2.6.0 (from -r /content/content/DAN/requirements.txt (line 19))\n","  Downloading google_auth-2.6.0-py2.py3-none-any.whl (156 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.3/156.3 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting google-auth-oauthlib==0.4.6 (from -r /content/content/DAN/requirements.txt (line 20))\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Collecting grpcio==1.44.0 (from -r /content/content/DAN/requirements.txt (line 21))\n","  Downloading grpcio-1.44.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting idna==3.3 (from -r /content/content/DAN/requirements.txt (line 22))\n","  Downloading idna-3.3-py3-none-any.whl (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from -r /content/content/DAN/requirements.txt (line 23)) (2.31.6)\n","Collecting importlib-metadata==4.11.1 (from -r /content/content/DAN/requirements.txt (line 24))\n","  Downloading importlib_metadata-4.11.1-py3-none-any.whl (17 kB)\n","Collecting isosurfaces==0.1.0 (from -r /content/content/DAN/requirements.txt (line 25))\n","  Downloading isosurfaces-0.1.0-py3-none-any.whl (10 kB)\n","Collecting joblib==1.2.0 (from -r /content/content/DAN/requirements.txt (line 26))\n","  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting kiwisolver==1.3.2 (from -r /content/content/DAN/requirements.txt (line 27))\n","  Downloading kiwisolver-1.3.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting mapbox-earcut==0.12.11 (from -r /content/content/DAN/requirements.txt (line 31))\n","  Downloading mapbox_earcut-0.12.11-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (98 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.4/98.4 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting Markdown==3.3.6 (from -r /content/content/DAN/requirements.txt (line 32))\n","  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.8/97.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting matplotlib==3.5.1 (from -r /content/content/DAN/requirements.txt (line 33))\n","  Downloading matplotlib-3.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting moderngl==5.6.4 (from -r /content/content/DAN/requirements.txt (line 34))\n","  Downloading moderngl-5.6.4-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_27_x86_64.whl (772 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.7/772.7 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting moderngl-window==2.4.1 (from -r /content/content/DAN/requirements.txt (line 35))\n","  Downloading moderngl_window-2.4.1-py3-none-any.whl (365 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.6/365.6 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multipledispatch==0.6.0 (from -r /content/content/DAN/requirements.txt (line 36))\n","  Downloading multipledispatch-0.6.0-py3-none-any.whl (11 kB)\n","Collecting networkx==2.6.3 (from -r /content/content/DAN/requirements.txt (line 37))\n","  Downloading networkx-2.6.3-py3-none-any.whl (1.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting numpy==1.22.2 (from -r /content/content/DAN/requirements.txt (line 38))\n","  Downloading numpy-1.22.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting oauthlib==3.2.0 (from -r /content/content/DAN/requirements.txt (line 39))\n","  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.5/151.5 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting opencv-python==4.5.5.62 (from -r /content/content/DAN/requirements.txt (line 40))\n","  Downloading opencv_python-4.5.5.62-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting packaging==21.3 (from -r /content/content/DAN/requirements.txt (line 41))\n","  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting progressbar (from -r /content/content/DAN/requirements.txt (line 42))\n","  Downloading progressbar-2.5.tar.gz (10 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting protobuf==3.19.4 (from -r /content/content/DAN/requirements.txt (line 43))\n","  Downloading protobuf-3.19.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyasn1==0.4.8 (from -r /content/content/DAN/requirements.txt (line 44))\n","  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyasn1-modules==0.2.8 (from -r /content/content/DAN/requirements.txt (line 45))\n","  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pycairo (from -r /content/content/DAN/requirements.txt (line 46))\n","  Downloading pycairo-1.25.1.tar.gz (347 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.1/347.1 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting pydub==0.25.1 (from -r /content/content/DAN/requirements.txt (line 47))\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Collecting pyglet==1.5.21 (from -r /content/content/DAN/requirements.txt (line 48))\n","  Downloading pyglet-1.5.21-py3-none-any.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting Pygments==2.11.2 (from -r /content/content/DAN/requirements.txt (line 49))\n","  Downloading Pygments-2.11.2-py3-none-any.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyparsing==3.0.7 (from -r /content/content/DAN/requirements.txt (line 50))\n","  Downloading pyparsing-3.0.7-py3-none-any.whl (98 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.0/98.0 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyrr==0.10.3 (from -r /content/content/DAN/requirements.txt (line 51))\n","  Downloading pyrr-0.10.3-py3-none-any.whl (46 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/content/DAN/requirements.txt (line 52)) (2.8.2)\n","Collecting pyunpack==0.2.2 (from -r /content/content/DAN/requirements.txt (line 53))\n","  Downloading pyunpack-0.2.2-py2.py3-none-any.whl (3.8 kB)\n","Collecting PyWavelets==1.2.0 (from -r /content/content/DAN/requirements.txt (line 54))\n","  Downloading PyWavelets-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting requests==2.27.1 (from -r /content/content/DAN/requirements.txt (line 55))\n","  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests-oauthlib==1.3.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/content/DAN/requirements.txt (line 56)) (1.3.1)\n","Collecting rich==11.2.0 (from -r /content/content/DAN/requirements.txt (line 57))\n","  Downloading rich-11.2.0-py3-none-any.whl (217 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m217.3/217.3 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting rsa==4.8 (from -r /content/content/DAN/requirements.txt (line 58))\n","  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n","Collecting scikit-image==0.19.2 (from -r /content/content/DAN/requirements.txt (line 59))\n","  Downloading scikit_image-0.19.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting scikit-learn==1.0.2 (from -r /content/content/DAN/requirements.txt (line 60))\n","  Downloading scikit_learn-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting scipy==1.8.0 (from -r /content/content/DAN/requirements.txt (line 61))\n","  Downloading scipy-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: screeninfo==0.7 in /usr/local/lib/python3.10/dist-packages (from -r /content/content/DAN/requirements.txt (line 62)) (0.7)\n","Requirement already satisfied: six==1.16.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/content/DAN/requirements.txt (line 63)) (1.16.0)\n","Collecting skia-pathops==0.7.2 (from -r /content/content/DAN/requirements.txt (line 64))\n","\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\")': /packages/02/eb/947aebddd41933ff3ed87c2d59defeaef308808993e05a3340c79276530e/skia_pathops-0.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33m  WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\")': /packages/02/eb/947aebddd41933ff3ed87c2d59defeaef308808993e05a3340c79276530e/skia_pathops-0.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33m  WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\")': /packages/02/eb/947aebddd41933ff3ed87c2d59defeaef308808993e05a3340c79276530e/skia_pathops-0.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33m  WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\")': /packages/02/eb/947aebddd41933ff3ed87c2d59defeaef308808993e05a3340c79276530e/skia_pathops-0.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\u001b[33m\n","\u001b[0m  Downloading skia_pathops-0.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting srt==3.5.1 (from -r /content/content/DAN/requirements.txt (line 65))\n","  Downloading srt-3.5.1.tar.gz (24 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting tqdm==4.62.3 (from -r /content/content/DAN/requirements.txt (line 73))\n","  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing_extensions==4.1.1 (from -r /content/content/DAN/requirements.txt (line 74))\n","  Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n","Collecting urllib3==1.26.8 (from -r /content/content/DAN/requirements.txt (line 75))\n","  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting watchdog==2.1.6 (from -r /content/content/DAN/requirements.txt (line 76))\n","  Downloading watchdog-2.1.6-py3-none-manylinux2014_x86_64.whl (76 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting Werkzeug==2.0.3 (from -r /content/content/DAN/requirements.txt (line 77))\n","  Downloading Werkzeug-2.0.3-py3-none-any.whl (289 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.2/289.2 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting zipp==3.7.0 (from -r /content/content/DAN/requirements.txt (line 78))\n","  Downloading zipp-3.7.0-py3-none-any.whl (5.3 kB)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.1->-r /content/content/DAN/requirements.txt (line 33)) (9.4.0)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.19.2->-r /content/content/DAN/requirements.txt (line 59)) (2023.9.26)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2->-r /content/content/DAN/requirements.txt (line 60)) (3.2.0)\n","Building wheels for collected packages: click-default-group, editdistance, srt, progressbar, pycairo\n","  Building wheel for click-default-group (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for click-default-group: filename=click_default_group-1.2.2-py3-none-any.whl size=3383 sha256=5e9756284b74d86ab442f3fb88f8b64771b1eeea7bce2b3fb367662efb01e8ff\n","  Stored in directory: /root/.cache/pip/wheels/2b/9c/67/f0b289bc6f573ea3269695a1b8f63846147e0e2f7140347181\n","  Building wheel for editdistance (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for editdistance: filename=editdistance-0.6.0-cp310-cp310-linux_x86_64.whl size=257199 sha256=b0fbcd525027f771764bb304b532f4f6cb26b09aad389c82238820744f48968d\n","  Stored in directory: /root/.cache/pip/wheels/d2/c2/60/295c59f8772b195d952c943f53ac4936e4ac59fc47a0030a1a\n","  Building wheel for srt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for srt: filename=srt-3.5.1-py3-none-any.whl size=22318 sha256=ab47e7691e020d247c29e70ef0cc8662338d6f4056b5a4c39288ec22e3164574\n","  Stored in directory: /root/.cache/pip/wheels/4e/92/77/b60b83fb496729f677716c1d1c77a339f435280bca58a9d409\n","  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for progressbar: filename=progressbar-2.5-py3-none-any.whl size=12067 sha256=1f382c127177565f91d37c9f2b3ba52f8b30df52737033bfdb93bfb98da47a9e\n","  Stored in directory: /root/.cache/pip/wheels/cd/17/e5/765d1a3112ff3978f70223502f6047e06c43a24d7c5f8ff95b\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for pycairo \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for pycairo (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n","\u001b[31m  ERROR: Failed building wheel for pycairo\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully built click-default-group editdistance srt progressbar\n","Failed to build pycairo\n","\u001b[31mERROR: Could not build wheels for pycairo, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install scipy\n","!pip install skia-pathops\n","!pip install tiffile\n","!pip install srt\n","!pip install skia-pathops\n","!pip install torch\n","!pip install  torchvision\n","!pip install screeninfo==0.7\n","!pip install pyunpack\n","#!pip install pycairo\n","!pip install -r /content/content/DAN/requirements.txt"]},{"cell_type":"markdown","source":["# **Dataset Creation**"],"metadata":{"id":"FjFvGpL_SR3h"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"nn-PLOANmqJz"},"outputs":[],"source":["%cd /content/content/DAN/Datasets/dataset_formatters\n","!python read2016_formatter.py"]},{"cell_type":"code","source":["#!zip -r /content/drive/MyDrive/DAN.zip /content/content/DAN"],"metadata":{"id":"VbKYFqDBn6WC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/content/DAN"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yIldZAdCv2Kb","executionInfo":{"status":"ok","timestamp":1700008992939,"user_tz":-630,"elapsed":529,"user":{"displayName":"Sanchi","userId":"16319292524849164681"}},"outputId":"370444c2-4262-4313-a0e4-cab095334cb2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/content/DAN\n"]}]},{"cell_type":"markdown","source":["# **Training**"],"metadata":{"id":"aHRujwwsNXJg"}},{"cell_type":"code","source":["import os\n","import sys\n","#DOSSIER_COURRANT = os.path.dirname(os.path.abspath(__file__))\n","#DOSSIER_PARENT = os.path.dirname(DOSSIER_COURRANT)\n","from torch.optim import Adam\n","from basic.transforms import aug_config\n","from OCR.ocr_dataset_manager import OCRDataset, OCRDatasetManager\n","from OCR.document_OCR.dan.trainer_dan import Manager\n","from OCR.document_OCR.dan.models_dan import GlobalHTADecoder\n","from basic.models import FCN_Encoder\n","from basic.scheduler import exponential_dropout_scheduler, linear_scheduler\n","import torch\n","import numpy as np\n","import random\n","import torch.multiprocessing as mp\n","\n","# function to train the model\n","def train_and_test(rank, params):\n","    torch.manual_seed(0)\n","    torch.cuda.manual_seed(0)\n","    np.random.seed(0)\n","    random.seed(0)\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True\n","\n","    params[\"training_params\"][\"ddp_rank\"] = rank\n","    model = Manager(params)\n","    model.load_model()\n","\n","    model.train()\n","\n","    # load weights giving best CER on valid set\n","    model.params[\"training_params\"][\"load_epoch\"] = \"best\"\n","    model.load_model()\n","\n","    metrics = [\"cer\", \"wer\", \"time\", \"map_cer\",  \"loer\"]\n","    for dataset_name in params[\"dataset_params\"][\"datasets\"].keys():\n","        for set_name in [\"test\", \"valid\", \"train\"]:\n","            model.predict(\"{}-{}\".format(dataset_name, set_name), [(dataset_name, set_name), ], metrics, output=True)\n","\n","\n","if __name__ == \"__main__\":\n","\n","    dataset_name = \"READ_2016\"  # [\"RIMES\", \"READ_2016\"]\n","    dataset_level = \"page\"  # [\"page\", \"double_page\"]\n","    dataset_variant = \"_sem\"\n","\n","    # max number of lines for synthetic documents\n","    max_nb_lines = {\n","        \"RIMES\": 40,\n","        \"READ_2016\": 30,\n","    }\n","\n","    params = {\n","        \"dataset_params\": {\n","            \"dataset_manager\": OCRDatasetManager,\n","            \"dataset_class\": OCRDataset,\n","            \"datasets\": {\n","                dataset_name: \"/content/drive/MyDrive/{}_{}{}\".format(dataset_name, dataset_level, dataset_variant),\n","            },\n","            \"train\": {\n","                \"name\": \"{}-train\".format(dataset_name),\n","                \"datasets\": [(dataset_name, \"train\"), ],\n","            },\n","            \"valid\": {\n","                \"{}-valid\".format(dataset_name): [(dataset_name, \"valid\"), ],\n","            },\n","            \"config\": {\n","                \"load_in_memory\": True,  # Load all images in CPU memory\n","                \"worker_per_gpu\": 4,  # Num of parallel processes per gpu for data loading\n","                \"width_divisor\": 8,  # Image width will be divided by 8\n","                \"height_divisor\": 32,  # Image height will be divided by 32\n","                \"padding_value\": 0,  # Image padding value\n","                \"padding_token\": None,  # Label padding value\n","                \"charset_mode\": \"seq2seq\",  # add end-of-transcription ans start-of-transcription tokens to charset\n","                \"constraints\": [\"add_eot\", \"add_sot\"],  # add end-of-transcription ans start-of-transcription tokens in labels\n","                \"normalize\": True,  # Normalize with mean and variance of training dataset\n","                \"preprocessings\": [\n","                    {\n","                        \"type\": \"to_RGB\",\n","                        # if grayscaled image, produce RGB one (3 channels with same value) otherwise do nothing\n","                    },\n","                ],\n","                \"augmentation\": aug_config(0.9, 0.1),\n","                # \"synthetic_data\": None,\n","                \"synthetic_data\": {\n","                    \"init_proba\": 0.9,  # begin proba to generate synthetic document\n","                    \"end_proba\": 0.2,  # end proba to generate synthetic document\n","                    \"num_steps_proba\": 200000,  # linearly decrease the percent of synthetic document from 90% to 20% through 200000 samples\n","                    \"proba_scheduler_function\": linear_scheduler,  # decrease proba rate linearly\n","                    \"start_scheduler_at_max_line\": True,  # start decreasing proba only after curriculum reach max number of lines\n","                    \"dataset_level\": dataset_level,\n","                    \"curriculum\": True,  # use curriculum learning (slowly increase number of lines per synthetic samples)\n","                    \"crop_curriculum\": True,  # during curriculum learning, crop images under the last text line\n","                    \"curr_start\": 0,  # start curriculum at iteration\n","                    \"curr_step\": 10000,  # interval to increase the number of lines for curriculum learning\n","                    \"min_nb_lines\": 1,  # initial number of lines for curriculum learning\n","                    \"max_nb_lines\": max_nb_lines[dataset_name],  # maximum number of lines for curriculum learning\n","                    \"padding_value\": 255,\n","                    # config for synthetic line generation\n","                    \"config\": {\n","                        \"background_color_default\": (255, 255, 255),\n","                        \"background_color_eps\": 15,\n","                        \"text_color_default\": (0, 0, 0),\n","                        \"text_color_eps\": 15,\n","                        \"font_size_min\": 35,\n","                        \"font_size_max\": 45,\n","                        \"color_mode\": \"RGB\",\n","                        \"padding_left_ratio_min\": 0.00,\n","                        \"padding_left_ratio_max\": 0.05,\n","                        \"padding_right_ratio_min\": 0.02,\n","                        \"padding_right_ratio_max\": 0.2,\n","                        \"padding_top_ratio_min\": 0.02,\n","                        \"padding_top_ratio_max\": 0.1,\n","                        \"padding_bottom_ratio_min\": 0.02,\n","                        \"padding_bottom_ratio_max\": 0.1,\n","                    },\n","                }\n","            }\n","        },\n","\n","        \"model_params\": {\n","            \"models\": {\n","                \"encoder\": FCN_Encoder,\n","                \"decoder\": GlobalHTADecoder,\n","            },\n","            # \"transfer_learning\": None,\n","            \"transfer_learning\": {\n","                # model_name: [state_dict_name, checkpoint_path, learnable, strict]\n","                \"encoder\": [\"encoder\", \"/content/drive/MyDrive/READ_2016_page/checkpoints/best.pt\", True, True],\n","                \"decoder\": [\"decoder\", \"/content/drive/MyDrive/READ_2016_page/checkpoints/best.pt\", True, False],\n","            },\n","            \"transfered_charset\": True,  # Transfer learning of the decision layer based on charset of the line HTR model\n","            \"additional_tokens\": 1,  # for decision layer = [<eot>, ], only for transfered charset\n","\n","            \"input_channels\": 3,  # number of channels of input image\n","            \"dropout\": 0.5,  # dropout rate for encoder\n","            \"enc_dim\": 256,  # dimension of extracted features\n","            \"nb_layers\": 5,  # encoder\n","            \"h_max\": 500,  # maximum height for encoder output (for 2D positional embedding)\n","            \"w_max\": 1000,  # maximum width for encoder output (for 2D positional embedding)\n","            \"l_max\": 15000,  # max predicted sequence (for 1D positional embedding)\n","            \"dec_num_layers\": 8,  # number of transformer decoder layers\n","            \"dec_num_heads\": 4,  # number of heads in transformer decoder layers\n","            \"dec_res_dropout\": 0.1,  # dropout in transformer decoder layers\n","            \"dec_pred_dropout\": 0.1,  # dropout rate before decision layer\n","            \"dec_att_dropout\": 0.1,  # dropout rate in multi head attention\n","            \"dec_dim_feedforward\": 256,  # number of dimension for feedforward layer in transformer decoder layers\n","            \"use_2d_pe\": True,  # use 2D positional embedding\n","            \"use_1d_pe\": True,  # use 1D positional embedding\n","            \"use_lstm\": False,\n","            \"attention_win\": 100,  # length of attention window\n","            # Curriculum dropout\n","            \"dropout_scheduler\": {\n","                \"function\": exponential_dropout_scheduler,\n","                \"T\": 5e4,\n","            }\n","\n","        },\n","\n","        \"training_params\": {\n","            \"output_folder\": \"dan_read_page\",  # folder name for checkpoint and results\n","            \"max_nb_epochs\": 50000,  # maximum number of epochs before to stop\n","            \"max_training_time\": 3600 * 24 * 1.9,  # maximum time before to stop (in seconds)\n","            \"load_epoch\": \"last\",  # [\"best\", \"last\"]: last to continue training, best to evaluate\n","            \"interval_save_weights\": None,  # None: keep best and last only\n","            \"batch_size\": 1,  # mini-batch size for training\n","            \"valid_batch_size\": 4,  # mini-batch size for valdiation\n","            \"use_ddp\": False,  # Use DistributedDataParallel\n","            \"ddp_port\": \"20027\",\n","            \"use_amp\": True,  # Enable automatic mix-precision\n","            \"nb_gpu\": torch.cuda.device_count(),\n","            \"optimizers\": {\n","                \"all\": {\n","                    \"class\": Adam,\n","                    \"args\": {\n","                        \"lr\": 0.0001,\n","                        \"amsgrad\": False,\n","                    }\n","                },\n","            },\n","            \"lr_schedulers\": None,  # Learning rate schedulers\n","            \"eval_on_valid\": True,  # Whether to eval and logs metrics on validation set during training or not\n","            \"eval_on_valid_interval\": 5,  # Interval (in epochs) to evaluate during training\n","            \"focus_metric\": \"cer\",  # Metrics to focus on to determine best epoch\n","            \"expected_metric_value\": \"low\",  # [\"high\", \"low\"] What is best for the focus metric value\n","            \"set_name_focus_metric\": \"{}-valid\".format(dataset_name),  # Which dataset to focus on to select best weights\n","            \"train_metrics\": [\"loss_ce\", \"cer\", \"wer\", \"syn_max_lines\"],  # Metrics name for training\n","            \"eval_metrics\": [\"cer\", \"wer\", \"map_cer\"],  # Metrics name for evaluation on validation set during training\n","            \"force_cpu\": False,  # True for debug purposes\n","            \"max_char_prediction\": 3000,  # max number of token prediction\n","            # Keep teacher forcing rate to 20% during whole training\n","            \"teacher_forcing_scheduler\": {\n","                \"min_error_rate\": 0.2,\n","                \"max_error_rate\": 0.2,\n","                \"total_num_steps\": 5e4\n","            },\n","        },\n","    }\n","\n","    if params[\"training_params\"][\"use_ddp\"] and not params[\"training_params\"][\"force_cpu\"]:\n","        mp.spawn(train_and_test, args=(params,), nprocs=params[\"training_params\"][\"nb_gpu\"])\n","    else:\n","        train_and_test(0, params)"],"metadata":{"id":"T1ULPUItNWaZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Predictions**"],"metadata":{"id":"U8VY8hUZPbln"}},{"cell_type":"markdown","source":["*Single lingual*"],"metadata":{"id":"caquKcxEZSXQ"}},{"cell_type":"code","source":["!cp /content/content/DAN/Datasets/dataset_formatters/generic_dataset_formatter.py /content/content/DAN\n","# Import necessary libraries\n","import os.path\n","\n","import torch\n","from torch.optim import Adam\n","from PIL import Image\n","import numpy as np\n","# Importing specific modules from custom packages\n","from basic.models import FCN_Encoder\n","from OCR.document_OCR.dan.models_dan import GlobalHTADecoder\n","from OCR.document_OCR.dan.trainer_dan import Manager\n","from basic.utils import pad_images\n","from basic.metric_manager import keep_all_but_tokens\n","\n","# Define a FakeDataset class for placeholder dataset information\n","class FakeDataset:\n","\n","    def __init__(self, charset):\n","        self.charset = charset\n","\n","        self.tokens = {\n","            \"end\": len(self.charset),\n","            \"start\": len(self.charset) + 1,\n","            \"pad\": len(self.charset) + 2,\n","        }\n","\n","# Function to get model parameters\n","def get_params(weight_path):\n","    return {\n","        \"dataset_params\": {\n","            \"charset\": None,\n","        },\n","        \"model_params\": {\n","            \"models\": {\n","                \"encoder\": FCN_Encoder,\n","                \"decoder\": GlobalHTADecoder,\n","            },\n","            # \"transfer_learning\": None,\n","            \"transfer_learning\": {\n","                # model_name: [state_dict_name, checkpoint_path, learnable, strict]\n","                \"encoder\": [\"encoder\", weight_path, True, True],\n","                \"decoder\": [\"decoder\", weight_path, True, False],\n","            },\n","            \"transfered_charset\": True,  # Transfer learning of the decision layer based on charset of the line HTR model\n","            \"additional_tokens\": 1,  # for decision layer = [<eot>, ], only for transfered charset\n","\n","            \"input_channels\": 3,  # number of channels of input image\n","            \"dropout\": 0.5,  # dropout rate for encoder\n","            \"enc_dim\": 256,  # dimension of extracted features\n","            \"nb_layers\": 5,  # encoder\n","            \"h_max\": 500,  # maximum height for encoder output (for 2D positional embedding)\n","            \"w_max\": 1000,  # maximum width for encoder output (for 2D positional embedding)\n","            \"l_max\": 15000,  # max predicted sequence (for 1D positional embedding)\n","            \"dec_num_layers\": 8,  # number of transformer decoder layers\n","            \"dec_num_heads\": 4,  # number of heads in transformer decoder layers\n","            \"dec_res_dropout\": 0.1,  # dropout in transformer decoder layers\n","            \"dec_pred_dropout\": 0.1,  # dropout rate before decision layer\n","            \"dec_att_dropout\": 0.1,  # dropout rate in multi head attention\n","            \"dec_dim_feedforward\": 256,  # number of dimension for feedforward layer in transformer decoder layers\n","            \"use_2d_pe\": True,  # use 2D positional embedding\n","            \"use_1d_pe\": True,  # use 1D positional embedding\n","            \"use_lstm\": False,\n","            \"attention_win\": 100,  # length of attention window\n","        },\n","\n","        \"training_params\": {\n","            \"output_folder\": \"dan_rimes_page\",  # folder name for checkpoint and results\n","            \"max_nb_epochs\": 50000,  # maximum number of epochs before to stop\n","            \"max_training_time\": 3600 * 24 * 1.9,  # maximum time before to stop (in seconds)\n","            \"load_epoch\": \"last\",  # [\"best\", \"last\"]: last to continue training, best to evaluate\n","            \"interval_save_weights\": None,  # None: keep best and last only\n","            \"batch_size\": 1,  # mini-batch size for training\n","            \"valid_batch_size\": 4,  # mini-batch size for valdiation\n","            \"use_ddp\": False,  # Use DistributedDataParallel\n","            \"ddp_port\": \"20027\",\n","            \"use_amp\": True,  # Enable automatic mix-precision\n","            \"nb_gpu\": torch.cuda.device_count(),\n","            \"ddp_rank\": 0,\n","            \"lr_schedulers\": None,  # Learning rate schedulers\n","            \"eval_on_valid\": True,  # Whether to eval and logs metrics on validation set during training or not\n","            \"eval_on_valid_interval\": 5,  # Interval (in epochs) to evaluate during training\n","            \"focus_metric\": \"cer\",  # Metrics to focus on to determine best epoch\n","            \"expected_metric_value\": \"low\",  # [\"high\", \"low\"] What is best for the focus metric value\n","            \"eval_metrics\": [\"cer\", \"wer\", \"map_cer\"],  # Metrics name for evaluation on validation set during training\n","            \"force_cpu\": True,  # True for debug purposes\n","            \"max_char_prediction\": 3000,  # max number of token prediction\n","            # Keep teacher forcing rate to 20% during whole training\n","            \"teacher_forcing_scheduler\": {\n","                \"min_error_rate\": 0.2,\n","                \"max_error_rate\": 0.2,\n","                \"total_num_steps\": 5e4\n","            },\n","            \"optimizers\": {\n","                \"all\": {\n","                    \"class\": Adam,\n","                    \"args\": {\n","                        \"lr\": 0.0001,\n","                        \"amsgrad\": False,\n","                    }\n","                },\n","            },\n","        },\n","    }\n","\n","#Function to make predictions using the trained model\n","def predict(model_path, img_paths):\n","    params = get_params(model_path)\n","    checkpoint = torch.load(model_path, map_location=\"cpu\")\n","    charset = checkpoint[\"charset\"]\n","    # Set models to evaluation mode\n","    manager = Manager(params)\n","    manager.params[\"model_params\"][\"vocab_size\"] = len(charset)\n","    manager.load_model()\n","    for model_name in manager.models.keys():\n","        manager.models[model_name].eval()\n","    manager.dataset = FakeDataset(charset)\n","\n","    # format images\n","    # Load and preprocess input images\n","    imgs = [np.array(Image.open(img_path)) for img_path in img_paths]\n","    imgs = [np.expand_dims(img, axis=2) if len(img.shape)==2 else img for img in imgs]\n","    imgs = [np.concatenate([img, img, img], axis=2) if img.shape[2] == 1 else img for img in imgs]\n","    shapes = [img.shape[:2] for img in imgs]\n","    reduced_shapes = [[shape[0]//32, shape[1]//8] for shape in shapes]\n","    imgs_positions = [([0, shape[0]], [0, shape[1]]) for shape in shapes]\n","    imgs = pad_images(imgs, padding_value=0, padding_mode=\"br\")\n","    imgs = torch.tensor(imgs).float().permute(0, 3, 1, 2)\n","\n","    # Prepare batch data for evaluation\n","    batch_data = {\n","        \"imgs\": imgs,\n","        \"imgs_reduced_shape\": reduced_shapes,\n","        \"imgs_position\": imgs_positions,\n","        \"raw_labels\": None,\n","    }\n","    # Perform evaluation on the batch\n","    with torch.no_grad():\n","        res = manager.evaluate_batch(batch_data, metric_names = [])\n","    prediction = res[\"str_x\"]\n","    # Define layout tokens for post-processing\n","    layout_tokens = \"\".join(['Ⓑ', 'Ⓞ', 'Ⓟ', 'Ⓡ', 'Ⓢ', 'Ⓦ', 'Ⓨ', \"Ⓐ\", \"Ⓝ\", 'ⓑ', 'ⓞ', 'ⓟ', 'ⓡ', 'ⓢ', 'ⓦ', 'ⓨ', \"ⓐ\", \"ⓝ\"])\n","    prediction = [keep_all_but_tokens(x, layout_tokens) for x in prediction]\n","    print(prediction)\n","    # Post-process predictions and write to a file\n","    with open('/content/content/DAN/prediction-single-lingual.txt','w') as f:\n","      f.write(prediction[0])\n","      f.close()\n","\n","if __name__ == \"__main__\":\n","    # Set the path to the pre-trained model and input image(s)\n","    model_path = \"/content/drive/MyDrive/DAN Model/dan_read_page.pt\"\n","    #img_paths = [\"../../../test.png\", \"../../../test2.png\"]  # CHANGE WITH YOUR IMAGES PATH\n","    img_paths = [\"/content/drive/MyDrive/handwritting-to-text-with-ocr.jpg\"]\n","    # Make predictions using the specified model and input image(s)\n","    predict(model_path, img_paths)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IyErDakEZgr5","executionInfo":{"status":"ok","timestamp":1700010363868,"user_tz":-630,"elapsed":7160,"user":{"displayName":"Sanchi","userId":"16319292524849164681"}},"outputId":"6e98bb42-0605-4db4-9ba2-73bed09c0caf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["##################\n","Available GPUS: 1\n","Rank 0: Tesla T4 _CudaDeviceProperties(name='Tesla T4', major=7, minor=5, total_memory=15101MB, multi_processor_count=40)\n","##################\n","Local GPU:\n","WORKING ON CPU !\n","\n","##################\n","transfered weights for encoder\n","transfered weights for decoder\n","LOADED EPOCH: -1\n","\n","['9h4Whenpast Calls,\\nanswer.Itnew new ']\n"]}]},{"cell_type":"markdown","source":["**Multilingual prediction**"],"metadata":{"id":"So-5Y4FZZJ97"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9gfy7rH60HBM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700010415895,"user_tz":-630,"elapsed":52039,"user":{"displayName":"Sanchi","userId":"16319292524849164681"}},"outputId":"28c742e8-f794-4fa8-9c49-8b04d1d96414"},"outputs":[{"output_type":"stream","name":"stdout","text":["##################\n","Available GPUS: 1\n","Rank 0: Tesla T4 _CudaDeviceProperties(name='Tesla T4', major=7, minor=5, total_memory=15101MB, multi_processor_count=40)\n","##################\n","Local GPU:\n","WORKING ON CPU !\n","\n","##################\n","transfered weights for encoder\n","transfered weights for decoder\n","LOADED EPOCH: -1\n","\n","['816Ir Eeie Catūat obt Ach Aūf Caber\\n\\nKax Er Bei Et 1 fr/ sti\\nCrisen.\\nD W E:\\nAzt .8']\n"]}],"source":["!cp /content/content/DAN/Datasets/dataset_formatters/generic_dataset_formatter.py /content/content/DAN\n","# Import necessary libraries\n","import os.path\n","\n","import torch\n","from torch.optim import Adam\n","from PIL import Image\n","import numpy as np\n","# Importing specific modules from custom packages\n","from basic.models import FCN_Encoder\n","from OCR.document_OCR.dan.models_dan import GlobalHTADecoder\n","from OCR.document_OCR.dan.trainer_dan import Manager\n","from basic.utils import pad_images\n","from basic.metric_manager import keep_all_but_tokens\n","\n","# Define a FakeDataset class for placeholder dataset information\n","class FakeDataset:\n","\n","    def __init__(self, charset):\n","        self.charset = charset\n","\n","        self.tokens = {\n","            \"end\": len(self.charset),\n","            \"start\": len(self.charset) + 1,\n","            \"pad\": len(self.charset) + 2,\n","        }\n","\n","# Function to get model parameters\n","def get_params(weight_path):\n","    return {\n","        \"dataset_params\": {\n","            \"charset\": None,\n","        },\n","        \"model_params\": {\n","            \"models\": {\n","                \"encoder\": FCN_Encoder,\n","                \"decoder\": GlobalHTADecoder,\n","            },\n","            # \"transfer_learning\": None,\n","            \"transfer_learning\": {\n","                # model_name: [state_dict_name, checkpoint_path, learnable, strict]\n","                \"encoder\": [\"encoder\", weight_path, True, True],\n","                \"decoder\": [\"decoder\", weight_path, True, False],\n","            },\n","            \"transfered_charset\": True,  # Transfer learning of the decision layer based on charset of the line HTR model\n","            \"additional_tokens\": 1,  # for decision layer = [<eot>, ], only for transfered charset\n","\n","            \"input_channels\": 3,  # number of channels of input image\n","            \"dropout\": 0.5,  # dropout rate for encoder\n","            \"enc_dim\": 256,  # dimension of extracted features\n","            \"nb_layers\": 5,  # encoder\n","            \"h_max\": 500,  # maximum height for encoder output (for 2D positional embedding)\n","            \"w_max\": 1000,  # maximum width for encoder output (for 2D positional embedding)\n","            \"l_max\": 15000,  # max predicted sequence (for 1D positional embedding)\n","            \"dec_num_layers\": 8,  # number of transformer decoder layers\n","            \"dec_num_heads\": 4,  # number of heads in transformer decoder layers\n","            \"dec_res_dropout\": 0.1,  # dropout in transformer decoder layers\n","            \"dec_pred_dropout\": 0.1,  # dropout rate before decision layer\n","            \"dec_att_dropout\": 0.1,  # dropout rate in multi head attention\n","            \"dec_dim_feedforward\": 256,  # number of dimension for feedforward layer in transformer decoder layers\n","            \"use_2d_pe\": True,  # use 2D positional embedding\n","            \"use_1d_pe\": True,  # use 1D positional embedding\n","            \"use_lstm\": False,\n","            \"attention_win\": 100,  # length of attention window\n","        },\n","\n","        \"training_params\": {\n","            \"output_folder\": \"dan_rimes_page\",  # folder name for checkpoint and results\n","            \"max_nb_epochs\": 50000,  # maximum number of epochs before to stop\n","            \"max_training_time\": 3600 * 24 * 1.9,  # maximum time before to stop (in seconds)\n","            \"load_epoch\": \"last\",  # [\"best\", \"last\"]: last to continue training, best to evaluate\n","            \"interval_save_weights\": None,  # None: keep best and last only\n","            \"batch_size\": 1,  # mini-batch size for training\n","            \"valid_batch_size\": 4,  # mini-batch size for valdiation\n","            \"use_ddp\": False,  # Use DistributedDataParallel\n","            \"ddp_port\": \"20027\",\n","            \"use_amp\": True,  # Enable automatic mix-precision\n","            \"nb_gpu\": torch.cuda.device_count(),\n","            \"ddp_rank\": 0,\n","            \"lr_schedulers\": None,  # Learning rate schedulers\n","            \"eval_on_valid\": True,  # Whether to eval and logs metrics on validation set during training or not\n","            \"eval_on_valid_interval\": 5,  # Interval (in epochs) to evaluate during training\n","            \"focus_metric\": \"cer\",  # Metrics to focus on to determine best epoch\n","            \"expected_metric_value\": \"low\",  # [\"high\", \"low\"] What is best for the focus metric value\n","            \"eval_metrics\": [\"cer\", \"wer\", \"map_cer\"],  # Metrics name for evaluation on validation set during training\n","            \"force_cpu\": True,  # True for debug purposes\n","            \"max_char_prediction\": 3000,  # max number of token prediction\n","            # Keep teacher forcing rate to 20% during whole training\n","            \"teacher_forcing_scheduler\": {\n","                \"min_error_rate\": 0.2,\n","                \"max_error_rate\": 0.2,\n","                \"total_num_steps\": 5e4\n","            },\n","            \"optimizers\": {\n","                \"all\": {\n","                    \"class\": Adam,\n","                    \"args\": {\n","                        \"lr\": 0.0001,\n","                        \"amsgrad\": False,\n","                    }\n","                },\n","            },\n","        },\n","    }\n","\n","#Function to make predictions using the trained model\n","def predict(model_path, img_paths):\n","    params = get_params(model_path)\n","    checkpoint = torch.load(model_path, map_location=\"cpu\")\n","    charset = checkpoint[\"charset\"]\n","    # Set models to evaluation mode\n","    manager = Manager(params)\n","    manager.params[\"model_params\"][\"vocab_size\"] = len(charset)\n","    manager.load_model()\n","    for model_name in manager.models.keys():\n","        manager.models[model_name].eval()\n","    manager.dataset = FakeDataset(charset)\n","\n","    # format images\n","    # Load and preprocess input images\n","    imgs = [np.array(Image.open(img_path)) for img_path in img_paths]\n","    imgs = [np.expand_dims(img, axis=2) if len(img.shape)==2 else img for img in imgs]\n","    imgs = [np.concatenate([img, img, img], axis=2) if img.shape[2] == 1 else img for img in imgs]\n","    shapes = [img.shape[:2] for img in imgs]\n","    reduced_shapes = [[shape[0]//32, shape[1]//8] for shape in shapes]\n","    imgs_positions = [([0, shape[0]], [0, shape[1]]) for shape in shapes]\n","    imgs = pad_images(imgs, padding_value=0, padding_mode=\"br\")\n","    imgs = torch.tensor(imgs).float().permute(0, 3, 1, 2)\n","\n","    # Prepare batch data for evaluation\n","    batch_data = {\n","        \"imgs\": imgs,\n","        \"imgs_reduced_shape\": reduced_shapes,\n","        \"imgs_position\": imgs_positions,\n","        \"raw_labels\": None,\n","    }\n","    # Perform evaluation on the batch\n","    with torch.no_grad():\n","        res = manager.evaluate_batch(batch_data, metric_names = [])\n","    prediction = res[\"str_x\"]\n","    # Define layout tokens for post-processing\n","    layout_tokens = \"\".join(['Ⓑ', 'Ⓞ', 'Ⓟ', 'Ⓡ', 'Ⓢ', 'Ⓦ', 'Ⓨ', \"Ⓐ\", \"Ⓝ\", 'ⓑ', 'ⓞ', 'ⓟ', 'ⓡ', 'ⓢ', 'ⓦ', 'ⓨ', \"ⓐ\", \"ⓝ\"])\n","    prediction = [keep_all_but_tokens(x, layout_tokens) for x in prediction]\n","    print(prediction)\n","    # Post-process predictions and write to a file\n","    with open('/content/content/DAN/prediction-single-lingual.txt','w') as f:\n","      f.write(prediction[0])\n","      f.close()\n","\n","if __name__ == \"__main__\":\n","    # Set the path to the pre-trained model and input image(s)\n","    model_path = \"/content/drive/MyDrive/DAN Model/dan_read_page.pt\"\n","    #img_paths = [\"../../../test.png\", \"../../../test2.png\"]  # CHANGE WITH YOUR IMAGES PATH\n","    img_paths = [\"/content/drive/MyDrive/multilingual.jpg\"]\n","    # Make predictions using the specified model and input image(s)\n","    predict(model_path, img_paths)\n"]},{"cell_type":"code","source":[],"metadata":{"id":"yDNU8iRs7ghc"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}